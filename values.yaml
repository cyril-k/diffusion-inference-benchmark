# Default values for inference-benchmark-backend.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
resultBucket: inference-benchmark-results
resultDir: acceptance-test-run-1-diffusion
replicaCount: 2
servedModel: &model THUDM/CogVideoX-5b

benchmarkSpecs:
  served_model: *model
  # batch_size_list: [1, 2, 4, 8, 16] to be used with H200
  batch_size_list: [1, 2, 4, 8]
  num_inference_steps_list: [20, 40, 60, 80, 100]

image:
  pullPolicy: Always
  serverURI: ghcr.io/cyril-k/videosys-server
  serverTag: latest
  loadGeneratorURI: ghcr.io/cyril-k/load-generator-videosys
  loadGeneratorTag: latest

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

podAnnotations: 
  prometheus.io/scrape: "true"
  # prometheus.io/port: "8080"

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # privileged: true


service:
  type: ClusterIP
  port: 8080

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

tolerations: []

hfToken: ""

objectStorageSecretName: model-storage-secret

sharedStorage:
  enable: false
  filestoreModelsDir: /mnt/data/model-storage
  size: 4Ti